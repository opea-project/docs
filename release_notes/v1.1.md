# OPEA Release Notes v1.1
We are pleased to announce the release of OPEA version 1.1, which includes significant contributions from the open-source community. This release addresses over 450 pull requests from xxx, enhancing xxxx.

More information about how to get started with OPEA v1.1 can be found at [Getting Started](https://opea-project.github.io/latest/index.html) page. All project source code is maintained in the [repository](https://github.com/opea-project). To pull Docker images, please access the [hub](https://hub.docker.com/u/opea).

## What's New in OPEA v1.1
This release introduces more scenarios with general availability, including the [Image-to-Video](https://github.com/opea-project/GenAIComps/pull/465), [Test-to-Image](https://github.com/opea-project/GenAIComps/pull/729), [Text-to-SQL](https://github.com/opea-project/GenAIExamples/tree/main/DBQnA) and [Avatar Animation](Animation microservice) capabilities; introduces [Prediction Guard](https://github.com/opea-project/GenAIComps/pull/677) to enhance AI security by Factuality Check, PII Detection, Prompt Injection Detection and Toxicity Detection; introduces [Generative AI Studio](https://github.com/opea-project/GenAIStudio) that streamlines the creation of enterprise Generative AI applications, offering an alternative to manual processes; expands the hardware support portfolio, like [Intel® Arc™ GPUs](https://github.com/opea-project/GenAIComps/pull/641) and [AMD® GPUs](https://github.com/opea-project/GenAIExamples/issues/1153); introduces enhanced monitoring system, providing real-time insights into performance and system resource utilization for [CPU and Intel® Gaudi® AI Accelerator](https://github.com/opea-project/GenAIEval/issues/195), as well as [Horizontal Pod Autoscaler(HPA)](https://github.com/opea-project/GenAIInfra/pull/531); integrates benchmark tools for long-context language models (LCLMs), like [LongBench](https://github.com/opea-project/GenAIEval/pull/179) and [HELMET](https://github.com/opea-project/GenAIEval/pull/182).

### Highlights
#### New GenAI Examples
- [AvatarChatbot](https://github.com/opea-project/GenAIExamples/tree/main/AvatarChatbot): a chatbot that combines a virtual "avatar" on either Intel Gaudi 2 AI Acclerator or Intel Xeon Scalable Processors.
- [DBQnA](https://github.com/opea-project/GenAIExamples/tree/main/DBQnA): it seamlessly translates natural language queries into SQL and delivers real-time database results.
- [EdgeCraftRAG](https://github.com/opea-project/GenAIExamples/tree/main/EdgeCraftRAG): a customizable and tunable RAG example for edge solutions on Intel® Arc™ GPUs.
- [GraphRAG](https://github.com/opea-project/GenAIExamples/tree/main/GraphRAG): a use case presents a Graph RAG approach to query-focused summarization.
- [Text2Image](https://github.com/opea-project/GenAIExamples/tree/main/Text2Image): an application that generates image conditioning on the provided texts. 
- [WorkflowExecAgent](https://github.com/opea-project/GenAIExamples/tree/main/WorkflowExecAgent): a workflow executor example to handle data/AI workflow operations via LangChain agents to execute custom-defined workflow-based tools.

#### New GenAI Components
- Text-to-Image: add [Stable Diffusion microservice](https://github.com/opea-project/GenAIComps/pull/729)
- Image-to-Video: add [Stable Video Diffusion microservice](https://github.com/opea-project/GenAIComps/pull/465)
- Text-to-SQL: add [Text-to-SQL microservice](https://github.com/opea-project/GenAIComps/pull/736)
- Text-to-Speech: add [GPT-SoVITS microservice](https://github.com/opea-project/GenAIComps/pull/784)
- Avatar Animation: add [Animation microservice](https://github.com/opea-project/GenAIComps/pull/775)
- RAG: add [GraphRAG with llama-index microservice](https://github.com/opea-project/GenAIComps/pull/793)

#### GenAIStudio
[GenAI Studio](https://github.com/opea-project/GenAIStudio), a new project of OPEA, streamlines the creation of enterprise Generative AI applications by providing alternate to manual processes with a seamless, end-to-end solution. From GenAI app development and evaluation to performance benchmarking and deployment, GenAI Studio empowers developers to effortlessly build, test, optimize their LLM solutions and create the deployment package. Its intuitive no-code/low-code interface accelerates innovation, enabling rapid development and deployment of cutting-edge AI applications with unparalleled efficiency and precision.

#### Prediction Guard
[Prediction Guard](https://docs.predictionguard.com) allows you to utilize hosted open access LLMs, LVMs, and embedding functionality with seamlessly integrated safeguards. In addition to providing a scalable access to open models, Prediction Guard allows you to configure factual consistency checks, toxicity filters, PII filters, and prompt injection blocking. 

#### Newly Supported Models
- llama-3.2 (1B/3B/11B/90B)
- glm-4-9b-chat
- Qwen2/2.5 (7B/32B/72B)

#### Newly Supported Hardware
- Intel® Arc™ GPU: [vLLM powered by OpenVINO](https://github.com/opea-project/GenAIComps/pull/729) can perform optimal model serving on Intel® Arc™ GPU.
- AMD® GPU: deploy GenAI examples on AMD® GPU using AMD® ROCm™: [CodeTrans](https://github.com/opea-project/GenAIExamples/pull/1138), [CodeGen](https://github.com/opea-project/GenAIExamples/pull/1130), [FaqGen](https://github.com/opea-project/GenAIExamples/pull/1126), [DocSum](https://github.com/opea-project/GenAIExamples/pull/1125), [ChatQnA](https://github.com/opea-project/GenAIExamples/pull/1122). 

### Notable Changes

<details><summary>GenAIExamples</summary> 
- x
</details>

<details><summary>GenAIComps</summary> 

- Functionalities
    - New microservices
        - Add stable diffusion microservice (#729)
        - Add image2video microservice (Stable Video Diffusion) (#465)
        - Text to SQL microservice (#736)
        - Add GPT-SoVITS microservice (#784)
        - Add image2image microservice (#794)
        - Initiate "animation" component (#775)
        - GraphRAG with llama-index (#793)
    - Enhanced microservices
        - Support Chinese for Docsum (#799)
        - Support file upload summary for DocSum microservice (#823)
        - MultimodalQnA Image and Audio Support Phase 1 (#852)
        - add dynamic batching embedding/reranking (#774)
        - refine codetrans prompt, support  parameter input (#822)
        - Update RAGAgentLlama and ReActLlama (#843)
        - Set a higher default value(1.2) about repetition_penalty for codegen example to reduce repetition (#820)
        - Add E2E Promeheus metrics to applications (#845)
        - [Agent] support custom prompt (#798)
        - support faqgen upload file in UI (#866)
    - Removed microservices
        - Remove useless vllm ray (#859)
    - Enhanced Security
        - Prediction Guard Guardrails components (#677)
        - Add WildGuard Guardrail Microservice (#710)
        - upgrade setuptools version to fix CVE-2024-6345 (#806)
        - Remote TGI/TGI services with OAuth Client Credentials authentication (#836)
    - Async support for microservices
        - Support async for embedding micorservice (#742)
        - TEI rerank microservice async support  (#746)
        - Async support for some microservices (#763)
    - Add RAG agent and ReAct agent implemention for llama3.1 served by TGI-gaudi (#722)
    - Support Llama3.2 vision and vision guard model (#753)
    - Enable vllm for Agent (#752)
    - Add Intel/toxic-prompt-roberta to toxicity detection microservice (#749)
    - Refactor milvus dataprep and retriever (#728)
- Performance
    - Fix vllm microservice performance issue.  (#731)
    - [Dataprep] Reduce Upload File Time Consumption (#744)
- New Hardware Support
    - Add vLLM ARC support with OpenVINO backend (#641)





- Enable bash scr to to be path-independent using $0 to address ERROR: failed to solve: failed to read dockerfile: open Dockerfile.intel_hpu: no such file or director when following README (#808)
- Add DPO support in finetuning microservice (#857)
- Combine CI/CD docker compose. (#861)
- vLLM support for Codegen (#886)
- agent short & long term memory with langgraph. (#851)
- Fix missing end of file chars (#874)
- quick fix (#894)
- Replace HTTP "inprogress" gauge with megaservice "request_pending" one (#864)
- Add support for Audio and Video summarization to Docsum (#865)
- vLLM support for FAQGen (#884)
- vLLM support for DocSum (#885)
- Block links that require real person verification (#896)
- Multiple models support for LLM TGI (#835)
- Block links that require real person verification (#897)
- Multiple models and remote service support for langchain vLLM text-generation (#887)
- Minor simplication to ServiceOrchestrator code (#889)
- Embedding compatible with OpenAI API (#892)
- Standardize the naming format of images (#898)
- fix history content from agent memory. (#899)
- Fix LLM special token issue (#895)
- trim input to TGI, moved clustering and summarization to dataprep and store in DB (#893)
- Bugfix for follow-up query with a .png image (#900)
- Docsum Gateway Fix (#902)
- vllm hpu fix version for bug fix (#903)
- add zero-shot vc readme (#904)
- Fix units of incorrect caption timestamps (#907)
- Add env for pass down model id in ChatQnA gateway (#906)
- Add "--no-verbose" flag to wget download commands in entrypoint (#909)
- Add empty list check (#914)
</details>

<details><summary>GenAIEvals</summary> 
- x
</details>

<details><summary>GenAIInfra</summary> 
- x
</details>

<details><summary>GenAIStudio</summary> 
- x
</details>

<details><summary>docs</summary>
- x
</details>