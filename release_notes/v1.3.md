# OPEA Release Notes v1.3
We are excited to announce the release of OPEA version 1.3, which includes significant contributions from the open-source community. This release addresses over 303(TODO: the number is changing) pull requests.

More information about how to get started with OPEA v1.3 can be found on the [Getting Started](https://opea-project.github.io/latest/index.html) page. All project source code is maintained in the [repository](https://github.com/opea-project). To pull Docker images, please access the [Docker Hub](https://hub.docker.com/u/opea). For instructions on deploying Helm Charts, please refer to the [guide](https://github.com/opea-project/GenAIInfra/tree/v1.3/helm-charts#readme).

## What's New in OPEA v1.3

This release introduces exciting capabilities, optimizations, and user-centric enhancements:

### Advanced Agent Capabilities
- <b>Multi-Turn Conversation</b>: Enhanced the OPEA agent framework for dynamic, context-aware dialogues. ([GenAIComps#1248](https://github.com/opea-project/GenAIComps/pull/1248)) 
- <b>Finance Agent Example</b>: a financial agent use-case [example](https://github.com/opea-project/GenAIExamples/FinanceAgent). ([GenAIExamples#1539](https://github.com/opea-project/GenAIExamples/pull/1539))

### Performance and Scalability
- <b>vLLM Enablement</b>: Integrated [vLLM](https://github.com/vllm-project/vllm) as the default LLM serving backend for key GenAI examples across Intel® Xeon® processors, Intel® Gaudi® accelerators, and AMD® GPUs. ([GenAIExamples#1436](https://github.com/opea-project/opea-project/GenAIExamples#1436))
- <b>OPEA Inference Microservice (Alpha)</b>: Simplified operating OPEA inference in cloud environment. (TODO: The feature name and scope may change.)
- <b>Horizontal Pod Scaling(HPA) for IaaS</b>: Added auto-scaling support for cloud-native deployments. (TODO:link [GenAIInfra#70??](https://github.com/opea-project/GenAIInfra/pull/770))

### Ecosystem Integrations
- <b>Haystack Integration</b>: Enabled OPEA as a backend of [Haystack](https://haystack.deepset.ai). ([Haystack-OPEA#1](https://github.com/opea-project/Haystack-OPEA#1))
- <b>Cloud Readiness</b>: [Azure](https://azure.microsoft.com) OPEA Endpoint (TODO: no PR yet). 

### New GenAI Capabilities
- <b>Text to Graph</b>: Creating graphs from text by extracting graph triplets. ([GenAIComps#1357](https://github.com/opea-project/GenAIComps/pull/1357))
- <b>Text to Cypher</b>: Generating and executing Cypher queries from natural language for graph database retrieval. ([GenAIComps#1319](https://github.com/opea-project/GenAIComps/pull/1319))
- <b>Enhanced Multimodality</b>: Added support for .mp3 files in [ASR](https://github.com/opea-project/GenAIComps/tree/main/comps/asr/src) and audio support for [MultimodalQnA](https://github.com/opea-project/GenAIExamples/tree/main/MultimodalQnA). ([GenAIExamples#1549](https://github.com/opea-project/GenAIExamples/issues/1549))

### Enhanced Evaluation
- <b>Enhanced Long-Context Model Evaluation</b>: Supported evaluating long-context model on Intel® Gaudi® with vLLM. ([HELMET#20](https://github.com/princeton-nlp/HELMET/pull/20))
- <b>TAG-Bench for SQL Agents</b>: Integrated [TAG-Bench](https://github.com/TAG-Research/TAG-Bench) to evaluate complex SQL query generation ([GenAIEval#230](https://github.com/opea-project/GenAIEval#230)).
- <b>Toxicity Detection Evaluation</b>: Introduced a workflow to evaluate the capability of detecting toxic language based on LLMs. ([GenAIEval#241](https://github.com/opea-project/GenAIEval/pull/241))
- <b>Model Card</b>: Added a model card generator for generating reports containing model performance and fairness metrics. ([GenAIEval#236](https://github.com/opea-project/GenAIEval/pull/236))
- <b>DocSum Support</b>: GenAIEval now supports evaluating the performance of DocSum.

### Observability
- <b>OpenTelemetry Tracing</b>: Leveraged OpenTelemetry to enable tracing for ChatQnA and AgentQnA on TGI and vLLM. ([GenAIExamples#1542](https://github.com/opea-project/GenAIExamples/issues/1542))

### Better User Experience
- <b>GenAIStudio</b>: Now supports drag-and-drop creation of agentic applications. (TODO: no PR yet)
- <b>Slimmed Docker Images</b>: Optimized Docker image sizes for faster deployment of GenAI examples. ([GenAIExamples#1585](https://github.com/opea-project/GenAIExamples#1585))
- <b>Documentation Refinement</b>: Refined the main README and hardware-specific READMEs for each example to help readers easily locate documentation tailored to deployment, customization, and hardware. ([GenAIExamples#1741](https://github.com/opea-project/GenAIExamples/issues/1741))


### Newly Supported Models
OPEA introduced the support for the following models in this release.

| Model                                       | TGI-Gaudi | vLLM-CPU | vLLM-Gaudi | Native |
| ------------------------------------------- | --------- | -------- | ---------- | ------ |
| [deepseek-ai/DeepSeek-V3]                   | ✓         | -        | ✓          | x      |
| [deepseek-ai/DeepSeek-R1-Distill-Llama-70B] | ✓         | -        | ✓          | x      |
| [deepseek-ai/DeepSeek-R1-Distill-Qwen-32B]  | ✓         | -        | ✓          | x      |
| [falcon-3.1-74b]                            | ✓         | ✓       | ✓           | x      |
| [ibm-granite/granite-3.2-8b-instruct]       | x         | -        | ✓           | x      |
| [llama-4] (TODO: model not yet released)    | ?         | ?        | ?            | ?     |
| [mistralai/Mistral-Small-24B-Instruct-2501] | ✓         | -        | ✓           | x      |
| [mistralai/Mistral-Large-Instruct-2411]     | x         | -        | ✓           | x      |
| [Phi-4-mini]                                | x         | -        | x           | ✓      |
| [Phi-4-multimodal-instruct]                 | x         | -        | x           | ✓      |

[All Supported LLM Models](https://github.com/opea-project/GenAIComps/tree/v1.3/comps/llms/src/text-generation#validated-llm-models)

### Newly Supported Hardware
- AMD® GPU using AMD® ROCm™ for AvatarChatbot, AudioQnA, DBQnA, SearchQnA. ([GenAIExamples#1288](https://github.com/opea-project/GenAIExamples/pull/1288), [GenAIExamples#1147](https://github.com/opea-project/GenAIExamples/pull/1147), [GenAIExamples#1273](https://github.com/opea-project/GenAIExamples/pull/1273), [GenAIExamples#1193](https://github.com/opea-project/GenAIExamples/pull/1193))

### Other Notable Changes (TODO: the list is changing)

Expand the follwing lists to read:

<details><summary>GenAIExamples</summary> 

- Functionalities
    - [AgentQnA] Add web search tool support and simplify the run instructions. (#1656) (e8f2313)
    - [ChatQnA] Add support for latest deepseek models on Gaudi (#1491) (9adf7a6)
    - [EdgeCraftRAG] A sleek new UI based on Vue and Ant Design for enhanced user experience, supporting concurrent multi-requests on vLLM, JSON pipeline configuration, and API-based prompt modification. (#1665) (5a50ae0)
    - [FaqGen] Merge FaqGen into ChatQnA for unified Chatbot experience. (#1654) (6d24c1c)

- Benchmark
    - [ChatQnA] Provide unified scalable deployment and benchmarking support for examples (#1315) (ed16308)

- Bug Fixes
    - [AgentQnA] Fix errors for running AgentQnA on xeon with openai and update readme (#1664) (fecc227)
    - [AudioQnA] Fix the LLM model field for inputs alignment (#1611) (2dfcfa0)

- Documentation
    - Update README.md for OPEA OTLP tracing (#1406) (4c41a5d)
    - Update README.md for Agent UI (#1495) (88a8235)
    - Refactor AudioQnA README (#1508) (9f36e84)
    - Add a new section to change LLM model such as deepseek based on validated model table in LLM microservice (#1501) (970b869)
    - Update README.md of AIPC quick start (#1578) (852bc70)

- CI/CD/UT
    - Added UT for rerank finetuning on Gaudi (#1472) (5f4b182)
    - Enable Gaudi 3, Rocm and Arc on manually release test. (#1615) (63b789a)
    - Enable base image build in CI/CD (#1669) (2204fe8)
<!--- Update TGI image versions (#1625) (1bd56af) -->

</details>

<details><summary>GenAIComps</summary> 

- Functionalities
    - [agent] enable custom prompt for react_llama and react_langgraph (#1391) (558a2f6)
    - [dataprep] Added Multimodal support for Milvus for dataprep component (#1380) (006bd91)
    - [cores/mega] remote endpoint support (#1399) (1871dec)
    - [docsum] Enlarge DocSum prompt buffer (#1471) (772ef6e)
    - [embeddings] Refine CLIP embedding microservice by leveraging the third-party CLIP (#1298) (7727235)
    - [guardrails] Add native support for toxicity detection guardrail microservice (#1258) (625aec9)   
    - [llm/text-generation] Add support for string message in Bedrock textgen (#1291) (364ccad)
    - [lvm] vLLM lvm integration (#1362) (831c5a3)
    - [nubula] Docker deployment support for Nebula graph database (#1396) (342c1ed)
    - [OVMS] Text generation, Embeddings and Reranking microservices based on [OVMS](https://github.com/openvinotoolkit/model_server) component (#1318) (78b94fc)
    - [text2image & image2image] enriched input parameters of text2image and image2image. (#1339) (42f323f)
    - Refine synchronized I/O in asynchronous functions (#1300) (b08571f)

- Bug Fixes
    - Docsum error by HuggingFaceEndpoint  (#1246) (30e3dea)
    - Fix tei embedding and tei reranking bug (#1256) (fa01f46)
    - Fix web-retrievers hub client and tei endpoint issue (#1270) (ecb7f7b)
    - Fix Dataprep Ingest Data Issue. (#1271) (b777db7)
    - Fix metric id issue when init multiple Orchestrator instance (#1280) (f8e6216)
    - Bug Fix neo4j dataprep ingest error handling and skip_ingestion argument passing (#1288) (4a90692)
    - Fix the retriever issue of Milvus (#1286) (47f68a4)
    - Fix Qdrant retriever RAG issue. (#1289) (c3c8497)
    - Fix agent message format. (#1297) (022d052)
    - Fix milvus dataprep ingest files failure (#1299) (a033c05)
    - Fix docker image security issues (#1321) (589587a)
    - Megaservice / orchestrator metric testing + fixes (#1348) (1064b2b)
    - Fix finetuning python regex syntax error (#1446) (380f95c)

- Documentation
    - GraphRAG README/compose fixes post refactor (#1221) (b38d9f3)
    - Update docs for LLamaGuard & WildGuard Microservice (#1259) (0df374b)
    - Fix Readme errors in dataprep component for all VectorDBs (#1377) (492f028)
    - Refine the README for llms/doc-summarization (#1437) (559ebb2)


- CI/CD/UT
    - Refine dataprep test scripts (#1305) (a4f6af1)

<!-- - Update nofile's hard limit to 262144 for opensearch (#1495) (de882e5) -->
</details>

<details><summary>GenAIEval</summary>

- Auto Tuner
    - RAG Pilot - A RAG pipeline tuning tool allowing fine-grained control over key aspects of parsing, chunking, postporcessing, and generating selection, enabling better retrieval and response generation. (#243) (97da8f2)

- Monitoring
    - Integrate with memory bandwidth exporter to support collection and reporting of memory bandwidth, cpu, mem metrics. (#218) (df5fd3e)

- Metrics
    - Collect vllm latency metric for e2e test (#244) (1b6a91d)

- Bug Fixes
    - Fix relative path issue for possion. (#234) (3b9981a)
    - Add the missed file in release package (#233) (28ed0db)
    - fix the error  of TTFT and TPOT while the bench target is chatqna_qlist_pubmed (#238) (da04a9f)
    - Fix performance benchmark with pubmed (#239) (5c8ab6e)

    <!-- - Collect vllm latency metric for e2e test (#244) (1b6a91d) -->
</details>

<details><summary>GenAIInfra</summary> 

- HelmChart
    - [TDX] Added Intel TDX support to helm charts (#799) (040860e)
    - Add helm starter chart for developing new charts (#776) (6154b6c)
    - HPA enabling usability improvement (#770) (3016f5f)
    - Helm chart for Ollama (#774) (7d66afb)
    - Helm: Add Qdrant support (#796) (99ccf0c)
    - Chatqna: Add Qdrant DB support (#813) (5576cfd)
    - Helm installed application metric Grafana dashboards (#800) (f46e8c1)
    - LLM TextGen Bedrock Support (#811) (da37b9f)

- CSP
    - Added automated provisioning of CosmosDB and App Insights for OPEA applications (#657) (d29bd2d)

- Bug Fixes
    - Fix the helm chart release dependency update (#842) (f121edd)

- CI/CD/UT
    - CI: enable milvus related test (#767) (5b2cca9)

</details>

<details><summary>GenAIStudio</summary> 

- update studio fe table UI and updated studio be according to the dataprep refactor (#32) (1168507)
- [Feat] Add GenAI Studio UI improvement (#48) (ad64f7c)
<!-- - Add GitHub Action to check and close stale issues and PRs (#49) (ef665f4) -->

</details>



## Deprecations
### Deprecated Examples
The following GenAI examples are deprecated, and will be removed since OEPA v1.3:
| Deprecated Example | Migration Solution | Reasons for Deprecation |
|--|--|--|
|[FaqGen](https://github.com/opea-project/GenAIExamples/tree/v1.2/FaqGen)|Use the example [ChatQnA](https://github.com/opea-project/GenAIExamples/tree/v1.3/ChatQnA) instead.| Provide users with a unified chatbot experience and reduce redundancy. |

### Deprecated Docker Images
The following Docker images are deprecated, and will be removed since OPEA v1.3:
| Deprecated Docker Image | Migration Solution | Reasons for Deprecation |
|--|--|--|
|[opea/chathistory-mongo-server](https://hub.docker.com/r/opea/chathistory-mongo-server)|Use [opea/chathistory-mongo](https://hub.docker.com/r/opea/chathistory-mongo) instead.| Follow the OPEA naming rules |
|[opea/faqgen](https://hub.docker.com/r/opea/faqgen)|Use [opea/chatqna](https://hub.docker.com/r/opea/chatqna) or [opea/chatqna-without-rerank](https://hub.docker.com/r/opea/chatqna-without-rerank) instead.| FaqGen is deprecated. |
|[opea/faqgen-ui](https://hub.docker.com/r/opea/faqgen-ui)|Use [opea/chatqna-ui](https://hub.docker.com/r/opea/chatqna-ui) instead.| FaqGen is deprecated. | 
|[opea/faqgen-react-ui](https://hub.docker.com/r/opea/faqgen-react-ui)|Use [opea/chatqna-ui](https://hub.docker.com/r/opea/chatqna-ui) instead.| FaqGen is deprecated. |
|[opea/feedbackmanagement](https://hub.docker.com/r/opea/feedbackmanagement)|Use [opea/feedbackmanagement-mongo](https://hub.docker.com/r/opea/feedbackmanagement-mongo) instead.| Follow the OPEA naming rules |
|[opea/promptregistry-mongo-server](https://hub.docker.com/r/opea/promptregistry-mongo-server)|Use [opea/promptregistry-mongo](https://hub.docker.com/r/opea/promptregistry-mongo) instead.| Follow the OPEA naming rules |

### Deprecated GenAIExample Variables
| Example | Type | Variable | Migration Solution |
|--|--|--|--|
|[ChatQnA](https://github.com/opea-project/GenAIExamples/tree/v1.3/ChatQnA)|environment variable|`your_hf_api_token`|Removed from Intel AIPC deployment. Use the environment variable `HUGGINGFACEHUB_API_TOKEN` instead. This change aligns with the standardized naming conventions for environment variables.|
|[ChatQnA](https://github.com/opea-project/GenAIExamples/tree/v1.3/ChatQnA)|environment variable|`OLLAMA_HOST`|Removed from Intel AIPC deployment. Instead, users can customize `LLM_SERVER_HOST_IP` in `ChatQnA/docker_compose/intel/cpu/aipc/compose.yaml`.|
|[DocIndexRetriever](https://github.com/opea-project/GenAIExamples/tree/v1.3/DocIndexRetriever)|environment variable|`TGI_LLM_ENDPOINT`|Removed due to no uses.|
|[DocIndexRetriever](https://github.com/opea-project/GenAIExamples/tree/v1.3/DocIndexRetriever)|environment variable|`MEGA_SERVICE_HOST_IP`|Removed due to no uses.|
|[DocIndexRetriever](https://github.com/opea-project/GenAIExamples/tree/v1.3/DocIndexRetriever)|environment variable|`LLM_SERVICE_HOST_IP`|Removed due to no uses.|
|[GraphRAG](https://github.com/opea-project/GenAIExamples/tree/v1.3/GraphRAG)|environment variable|`MAX_OUTPUT_TOKENS`|Instead, it has been split into two new environment variables: `MAX_INPUT_TOKENS` (default: 4096) and `MAX_TOTAL_TOKENS` (default: 8192) to control the maximum token limits.|

### Deprecated GenAIComps Parameters
| Component | Parameter | Migration Solution |
|--|--|--|
|[agent](https://github.com/opea-project/GenAIComps/tree/v1.3/comps/agent)|`with_store` of `agent_config` in the Assistants APIs|Its functionality is now fully covered by the new `memory_type` parameter. In v1.3, please use `"with_memory": true` and `"memory_type": persistent` as its replacement. The `with_memory` parameter in `agent_config` of APIs is now enabled by default (true) for enabling multi-turn conversations. Please refer to the [guide](https://github.com/opea-project/GenAIComps/blob/v1.3/comps/agent/src/README.md#15-agent-memory) for more details.|

## Dependencies Updates (TODO: update after some bug fixes)

| Dependency | Hardware | Scope | Version | Version in OPEA v1.2 | Comments |
|--|--|--|--|--|--|
|gradio|-|all examples|5.11.0|5.5.0||
|huggingface/text-embeddings-inference|all|all examples|cpu-1.6|cpu-1.5||
|huggingface/text-generation-inference|Xeon|all examples|2.4.1|2.1.0||
|huggingface/text-generation-inference|Gaudi|MultimodalQnA|2.3.1|2.0.6|Other examples have upgraded TGI in v1.2.|
|huggingface/text-generation-inference|AMD GPU|all examples|2.4.1-rocm|2.3.1-rocm||
|langchain<br/>langchain_community|-|llms/doc-summarization<br/>llms/faq-generation|0.3.14|0.3.15|Avoid bug in FaqGen and DocSum.|
|optimum-habana|Gaudi|lvms/llama-vision|1.15.0|-||
|pytorch|Gaudi|lvms/llama-vision|2.5.1|2.4.0||
|transformers|-|lvms/llama-vision|4.48.0|4.45.1||
|vllm|Xeon|all examples|latest stable|latest||
|langchain-chroma|-|web_retrievers|latest|-||
|langchain-google-community|-|web_retrievers|latest|-||

## Changes to Default Behavior
- [[agent](https://github.com/opea-project/GenAIComps/tree/v1.3/comps/agent)] The default model changed from `meta-llama/Meta-Llama-3-8B-Instruct` to `meta-llama/Llama-3.3-70B-Instruct`.


## Known Issues
- [AvatarChatbot](https://github.com/opea-project/GenAIExamples/tree/v1.3/AvatarChatbot) can not work in K8s environment because of a functional gap in wav2clip service. ([GenAIExamples#1506](https://github.com/opea-project/GenAIExamples/pull/1506))

## Full Changelogs
- GenAIExamples: [v1.2...v1.3](https://github.com/opea-project/GenAIExamples/compare/v1.2...v1.3)
- GenAIComps: [v1.2...v1.3](https://github.com/opea-project/GenAIComps/compare/v1.2...v1.3)
- GenAIInfra: [v1.2...v1.3](https://github.com/opea-project/GenAIInfra/compare/v1.2...v1.3)
- GenAIEval: [v1.2...v1.3](https://github.com/opea-project/GenAIEval/compare/v1.2...v1.3)
- GenAIStudio: [v1.2...v1.3](https://github.com/opea-project/GenAIStudio/compare/v1.2...v1.3)
- docs: [v1.2...v1.3](https://github.com/opea-project/docs/compare/v1.2...v1.3)

## Validated Hardware
- Intel® Gaudi® Al Accelerators (2nd, 3rd)
- Intel® Xeon® Scalable processor (4th, 5th)
- Intel® Arc™ Graphics GPU (A770)
- AMD® ROCm™ (MI300X)

## Validated Software
- Ubuntu 22.04
- Habana [v1.20](https://docs.habana.ai/en/v1.20.1/Installation_Guide/index.html)
- Docker 28.0.4
- Docker Compose v2.34.0
- Kubernetes v1.29.15


## Contributors
This release would not have been possible without contributions of the following organizations and individuals.

### Contributing Organizations (TODO: the list is changing)
- `Amazon`: Ollama deployment, Bedrock integration, OVMS integration and bug fixes.
- `AMD`: vLLM enablement on AMD GPUs for key examples, AMD GPUs enabling on more examples, AMD OPEA blogs.
- `Intel`: Development and improvements to GenAI examples, components, infrastructure, and evaluation.
- `Infosys`: Azure support and documentation updates.
- `National Chiao Tung University`: Documentation updates.

### Individual Contributors
For a comprehensive list of individual contributors, please refer to the [Full Changelogs](#full-changelogs) section.
